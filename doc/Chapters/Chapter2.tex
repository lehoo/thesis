% Chapter 2

\chapter{Literature Review}
\label{Chapter2}
\lhead{Chapter 2. \emph{Literature Review}}

In the literature review, we discuss the area of research covered by
the thesis, citing relevant papers and articles that serve as a base
of ideas for this document.\\
First, we talk about the Message-Passing Interface (MPI), the parallel
programming API used for the purposes of this thesis. We also talk
about the different implementations of this API, notably OpenMPI and
MPICH. We discuss the methods of modelling and simulation in general.
Then, we talk about SimGrid, which is a simulation-based
framework, and SMPI, the implementation of MPI that runs on top of
SimGrid. While discussing SMPI, we present the idea of a framework
that would make it possible to automate running tests, trace
collection and post-processing. In connection to this framework, we
talk about workflows as a means to automate our experiments and
whether scientific or business workflows are more suitable for
us. We also talk about the importance of reproducible research, which
is another argument for constructing such a framework.
\section{MPI}
Distributed computing is a very active and important subject of research
in computer science, including fields such as cluster computing, grid
computing, Cloud computing, or peer-to-peer computing. Communication
between the different processes in a distributed application can be
implemented in a number of ways. As communication is necessary in most
cases, a standardized communication protocol can be a lot of help when
developing a distributed program. The Message-Passing Interface (MPI)
is a language-independent message-passing library
interface specification. It is not a language, but a standard - there
exist multiple MPI implementations. Since its take-off, it has become
a de facto standard for inter-process communication. The standard
provides vendors a clear set of routines, that they can implement
efficiently, or in a way that it suits the hardware they
provide.\cite{mpif12}
\subsection{OpenMPI}
OpenMPI is an MPI implementation with the goal of being able to
achieve good performance on a wide range of different aspects of
high-performance computing. To efficiently support multiple types of
parallel machines, high performance “drivers” for all established
interconnects are developed. These include TCP/IP, shared memory,
Myrinet, Quadrics, and Infiniband. Features for checking data
integrity are provided in order to account for network transmission
errors. With the utilization of message fragmentation and striping
over multiple (potentially heterogeneous) network devices, OpenMPI
provides an increased bandwidth to applications, as well as the
ability to handle the failure of network devices during
runtime.\cite{ompi04} On the Grid'5000 cluster, which is used to most
of the research conducted for this thesis, OpenMPI is the default MPI
implementation used by the default images.
\subsection{MPICH}
MPICH was originally developed during the MPI standards process
starting in 1992 to provide feedback to the MPI Forum on
implementation and usability issues. This original implementation was
based on the Chameleon portability system to provide a light-weight
implementation layer (hence the name MPICH from MPI over
CHameleon). Around August 2001, development begun on a new
implementation called MPICH2.\cite{mpich12} This implementation
introduced improvements on collective communication operations by
using multiple algorithms, choosing between them depending on certain
variables - for example the message size.\cite{trg05} Another
important result during the development of MPICH2 was the design of
the Nemesis communication subsystem and the porting of MPICH2 on that
system. The efficient implementation of shared-memory communication
helped Nemesis MPICH2 achieve low latency and high
bandwidth.\cite{bmg07}\\
Starting with November 2012, the project is renamed to MPICH, with
version number 3.0.\cite{mpich12}
\section{Modelling and Simulation}
In distributed computing, modelling means creating an abstraction of a
real system by taking only the aspects of it that are relevant to the
system's behavior into account. Once constructed, such a model becomes
a tool with which we can investigate the behavior of the
system.\cite{h12_1}
\subsection{Advantages of Modelling}
Modelling and simulation techniques have been used extensively in
parallel computing and is an ongoing research topic, with new
challenges continuously arising. There are various reasons for its
importance.\\
Conducting experiments on real-world systems can be
infeasible because experimenting would disrupt the service that is
provided by the system. For example, in the case of a mail server,
experiments or monitoring could cause delay, or maybe even data
loss. Service disruption can sometimes be even dangerous, in addition
to being an inconvenience: in the case of a nuclear reactor, delay or
loss of data can prove fatal. Timeliness can be as important in such
systems as correctness. However, performance analysis and monitoring
might be crucial to draw conclusions about maintenance, for
example. Another problem with direct experimentation is that the
information we are looking for may not be available, or may be
complicated to get. For example, in most operating systems, it is
difficult to obtain the exact timing of instruction-level
events.\cite{h12_1} Also, when conducting experiments on a real-world
system, results are often non-reproducible, due to resource
dynamics.\cite{clq08} Another argument on the side of modelling is
that it provides the ability of experimenting on different
configurations. Investing in a large-scale computer cluster, or the
setup of a distributed grid environment is an expensive and tedious
process. Investors want to make sure that they get what they
want: they impose performance constraints on the system. This means
that they want to know how the system will behave
before buying it and setting it up. To predict the behavior,
experiments are needed to be conducted. We need to do these
experiments on different setups, before finding out which one is the
best in the current situation. Changing the hardware or software
configuration parameters on a real-world system is very inconvenient -
in most cases, it's not doable, because of time and money
constraints. Thus, the solution is to simulate the desired
system, and run the experiments there. This way, changing the
configuration is simple and costless.\cite{h12_1} Another great
benefit of simulation is that in a classroom setting, students can
learn the principles of high-performance and distributed computing
without actual access to a parallel platform.\cite{csgscq11}
\subsection{Analytical and Simulation Models}
The accuracy of a model can vary: we can make an analytical, or
qualitative model, in which all definite values are abstracted away -
in this case, we get a representation of
the system, which can be analysed mathematically to deduce its
behavior. When using this method, no experiments can be conducted, we
solely rely on theoretical analysis. In contrast, a simulation model
is a stochastic model, which is an algorithmic abstraction of the
real-world system that can be executed to reproduce the system's
behavior. This model is also called a quantitative model, as we can
get estimates of the modeled system's quantitative attributes, such as
response time or throughput. In other words, we can use a simulation
model to conduct performance analysis on a system, without actually
having the actual system at our disposal.\cite{h12_1}\cite{h12_13}\\
When wanting to get a prediction about how a specific system would
perform, a theoretical model, in most cases, produces unreliable and
unrealistic results - it's not feasible for such accurate
predictions. The vast majority of research results are obtained via
empirical evaluation of experiments.\cite{clq08} For these reasons, we
use the simulation model in this thesis. As we stated before, such a
model can be executed, which is called simulation. During simulation,
the model is supposed to behave like the real system would. It is hard
to produce a 100\% accurate simulation, but more and more reliable
solutions are being developed. The simulation model contains
more aspects of the real system compared to the theoretical model, in
order to accurately represent the system, while still avoiding
unnecessary detail.\cite{h12_1} Creating and executing a simulation
model is complicated, computationally expensive and poses a number of
challenges, thus, a good simulation framework (such as SMPI) can prove
to be of much help when conducting experiments.
\section{Off-line and partial on-line simulation}
Full simulation - including CPU and network emulation - of a parallel
application can be, in many cases, even more resource-intensive than
running real-world experiments. This contradicts the fact that one of
the most prominent goals of simulation is to observe the behavior of
such large-scale platforms that aren't available. Thus, there is much
interest in more efficient simulation approaches. \cite{bdglmqssv13}
The most widely used of such approaches fall into two categories:
off-line simulation, which is also called trace-based or post-mortem
simulation and on-line simulation, which is simulation via direct
execution.\cite{csgscq11} As in the subject of this thesis, we are
interested only in the simulation of MPI applications, we describe
the two different simulation approaches concentrating specifically on
that subject.
\subsection{Off-line simulation}
For conducting off-line simulation, logs or traces are needed to be
collected of an execution of the MPI application to be simulated,
taking place on a
real-world platform. This is necessary because the obtained traces are
used as an input for the simulator, which then replays the execution
traces as if the application was running on the target platform. This
platform's characteristics may differ from the one's that we obtained
the traces from, since we may want to use the simulator to predict the
application's performance on a different system. Thus, there is a need
to calculate how the target platform would execute the application,
based on the traces we got on the other platform. The typical approach
to this problem is to first compute the time intervals between the MPI
communication operations. During these intervals, local computations
were conducted, that's why we call these "CPU bursts". During
simulation, we have to account for the differences between the
performance of the platforms by modifying the time these CPU bursts
take. This can be done by simply scaling the time intervals, or by
using more sophisticated methods, by calculating exactly how the
application's computational signature and the platform's hardware
signature relate.\cite{csgscq11} Communication operations, of course,
also need to be simulated. This is done based on the events recorded
on the trace, and on the network model of the simulated
platform.\cite{csgscq11}\\
As mentioned in \cite{csgscq11}, there are multiple downsides and
challenges to the off-line approach. On such downside is that when
wanting to simulate a relatively larger-scale application, the size of
the obtained traces can be so large, that running the simulation on a
single node might become a problem. Methods in order to overcome this
obstacle include a compact representation of the traces in order to
reduce its size. Another solution is to only consider a carefully
selected subset of the obtained traces. A big disadvantage when using
off-line simulation is that because we use the traces as an input to
the simulator in order to replay the execution of the application, the
simulation is dependant on the platform we collect the traces
on. This means that, for example, there can be features in the
obtained traces that might not be available on the target platform. In
most cases, it is also necessary that the two platforms have the same
number of nodes to run the experiment on. Although there has been a
good amount of research done in the area, MPI itself and also the
application might alter its behavior depending on problem and message
size. Because of this, simulating the scaling of an application is a
very hard, if not impossible task.\cite{bdglmqssv13}
\subsubsection{Time-independent traces}
Another link that ties the produced trace to the host platform occurs
when we use timed traces, meaning that each traced event is associated
to a time-stamp. Since the time delays between the events are specific
to the platform specification, the simulator has to apply a correction
factor to these delays when running the simulation on the target
platform. Thus, the simulator has to know precisely the specification
of both the host and the target platform, in order to be able to
calculate this correction factor. Another difficulty regarding that
comes up regarding this problem is that actually calculating the
correction factor is a tedious process. It can take a considerable
amount of time, depending on how similar the host and the target
platforms are.\cite{dmsq11}\\
In \cite{dmsq11}, a solution to this problem is proposed:
time-independent traces. Acquiring time-independent traces means that
the traces won't contain any timestamps, breaking this link between
the acquisition and the replay of the traces. In these type of traces,
for each computation or communication operation, we log the volume of
the operation (in number of floating point operations or bytes)
instead of the time the execution took. This type of information, in
most cases, does not vary depending on the platform the experiment is
run on. The exceptions are the adaptive MPI applications that modify
their execution path according to the execution platform.\\
\cite{ms11} contains a guide describing how to acquire such traces on
the Grid5000 platform. The guide was used to serve as a base for the
process on how to acquire traces. Since the work in this thesis is
mostly related to producing traces for validating on-line simulation,
in which case time-stamps don't have an influence on the process
(neither in a positive, nor a negative way), the extraction of
time-independent information from the traces can be omitted in our
case.
\subsection{Partial on-line simulation}
Partial on-line simulation is a different approach. Here, we execute
the program with no or very little modification on a host platform,
that tries to mimic the behavior of the target
platform.\cite{csgscq11} Computational tasks are executed on the
hardware, but the timing and the delivery of the messages is
calculated by the simulation environment. Thus, the simulator is
responsible for mainaining the correct order of the events, both
computational and communicational.\cite{bdglmqssv13}\\
A downside of the on-line approach is that since we actually execute
the code, the resource needs for running the simulation is about as
high or even higher (in case of needing an extra node to run the
simulation component, for example) than it is for the actual
experiment. Techniques have been implemented in order to help
alleviate this problem. The basic idea is that
the actual results of the experiments (for example, the result of
multiplying two matrices) might not be important in our case: we are
only interested in the \emph{time} it takes to get those results on
the target platform. This is why methods can be employed which trade
off accuracy for performance. This idea might not be feasible for
experiments where data-dependent application behavior is
vital, but a large portion of benchmarks can be indeed simulated
this way, providing a reasonably accurate execution profile.\\
Although slower, on-line simulation is more general than the off-line
approach, as it does not, in any way depend on some other platform -
whereas in the case of off-line simulation, as we mentioned before,
the trace is acquired on a different platform, with maybe specific
application configurations, thus inevitably bringing dependencies.
\section{SimGrid}
For reasons mentioned before, simulation techniques have historically
been widely utilised in several areas of computer science,
e.g. microprocessor design, network protocol design. Due to this, a
lot of effort went into developing the technology and as a result,
widely used and reliable simulation frameworks have been developed in
these areas. However, there hasn't been a well-developed standard
simulation tool for what we talk about in this thesis: execution of
distributed applications on distributed computing platforms. Rather,
there has only been a number of in-house developed, highly specialized
tools to satisfy the need of the community. SimGrid is a more generic
simulation framework that is being developed to be one of the
acknowledged and widespread tools for simulation in large-scale
distributed computing.\cite{clq08}\\
SimGrid's key features include:\cite{clq08}
\begin{itemize}
\item A scalable and extensible simulation engine that implements
  several validated simulation models, and that makes it possible to
  simulate arbitrary network topologies, dynamic computational and
  network resource availabilities, as well as resource failures;
\item High-level user interfaces for researchers (who are not
  necessarily computer science experts, but rather experts on their own
  field of research) to quickly assemble simulation prototypes in either
  C or Java;
\item APIs for distributed computing developers to create distributed
  applications that can run seamlessly in either "simulation mode" or
  "real-world mode", in order to be able to test it on the simulated
  environment before actually deploying it.
\end{itemize}
SimGrid is a very active project, both in terms of research and in
terms of development. It is a favored tool by researchers, which is
proven by the increasing number of papers written where the research
was conducted using SimGrid as a scientific instrument. In terms of
development, the developer team envisions a number of directions for
future work: addition of a model for disk resources; extention of
scalability to improve usability in the P2P domain; ability to
dispatch simulated nodes over several physical machines.\cite{clq08}
Another important field of research for the SimGrid team is the
implementation of the API that has already been mentioned: the
Message-Passing Interface (MPI).
\section{SMPI}
As stated before, MPI is one of the most widely used APIs for
communication between nodes in distributed computing. SMPI is a
framework for simulating on a single node the execution of parallel
applications implemented using the MPI standard. It is part of the
SimGrid project and as such, it is built on the SimGrid simulation
kernel, benefiting from its fast, scalable and validated network
models. SMPI also extends the existing model with other techniques,
such as a validated piece-wise linear model for data transfer times
between cluster nodes. SMPI simulations also account for network
contention - timing and delivery of the messages are determined using
the network model of SimGrid.\cite{csgscq11} A current limitation in
SMPI is that it is unable to simulate high-performance networking
hardware such as Infiniband. Thus, when wanting to compare simulation
to real-life results, we have to make sure those results were gatheres
using Gigabit ethernet.\\
Three of the main challenges for simulating an MPI application are:
\begin{itemize}
\item Accuracy: The prediction of the real-world execution time (the
  "simulated time") needs to be as accurate as possible, so that
  reasonable conclusions can be drawn from the experiments.
\item Scalability: We want to be able to simulate large-scale
  applications within a reasonable timescale.
\item Speed: It would be advantageous if the simulation time (the
  actual time of running the simulation) would be as low as possible,
  compared to the simulated time (the predicted execution time of the
  real-world application).
\end{itemize}
As for simulation methods, SMPI can be used for both off-line and
on-line simulation, although the emphasis is more on the on-line
approach, since it's actually a partial implementation of the MPI
standard in itself, thus making it feasible for executing MPI
experiments. More specifically, in SMPI, the goal is to be able to
make such simulations on a single node. The most prominent challenges
when doing this are the large CPU and memory requirements. SMPI
provides some special techniques that help overcoming these
challenges. The basic idea about trading off accuracy for performance
has already been described in the previous section about on-line
simulation. SMPI implements multiple such techniques, allowing to run
experiments with such high resource requirements that would otherwise
be impossible to fulfill. Such a method in order to reduce CPU usage
is to run the
benchmark only on a subset of all the nodes, while in place of running
the code on the others as well, we just insert the computation time
that we got previously. Apart from CPU usage, we need to also account
for the need for memory. A technique for that is "RAM folding": here,
multiple simulated processes, that in SMPI are, in fact, simple
threads, use the same reserved memory location, thus overwriting each
other's data structures. Also, another implemented solution is to
remove large data array references from the code, with the help of the
compiler which can result in the complete removal of potentially
large, now unreferenced arrays. Again, this obviously corrupts the
results that the experiment program gives, but in the same time helps
to simulate applications that would use such an amount of memory that
just wouldn't be physically possible to provide in our testing
environment, while still providing a reliable estimate of the
performance.\cite{bdglmqssv13} These features are disabled by default,
they have to be explicitly enabled by the user.\\
Extensive testing was conducted in \cite{csgscq11} to verify the
previously mentioned qualities of the framework. In these tests, the
OpenMPI and MPICH implementations were used to serve as verification
benchmarks: the same experiments were run using both MPI
implementations, as well as simulated with SMPI. The
results show that SMPI predicted the execution time of OpenMPI and
MPICH applications for point-to-point, one-to-many and many-to-many
applications with an average error value of under 10\% in each
cases. Using the aforementioned techniques to reduce the memory
footprint, SMPI tests
were successfully conducted on a scale of up to 448 processors. The
results showed that the predicted execution times were
underestimates with an average error value of 18.5\%, which is higher
than in previous experiments without these techniques. We have to note
here, though, that
certain tests weren't successful without the RAM-folding techniques,
due to an out-of-memory error. This shows, that although it poses
difficulties, reducing the memory usage is vital in SMPI.\\
As SMPI is an actively developed project alongside SimGrid, there are
a number of research directions. One major development to the
project would be a testing framework that would aim to lessen the
burdens of testing as much as possible. The goal is to provide a
unified method to set up experiments across different environments and
to do it with as little necessary adjustments on user part as
possible.
\section{Experimentation tools}
Experiment orchestration and process automation is not a new
idea. There exist multiple tools for doing this. Among others, such
tools are Expo\cite{vr08}, Plush, OMF\cite{rosj09} or the
Grid5000-specific g5k-campaign. The problem is that most of these
tools use a bottom-up design, meaning that in order to understand and
use the experiments, the user has to understand the lower-level
building blocks first. A top-down approach would make it possible to
start the design of the experiment with a high-level description, then
work our way down to the lower level details while implementing. There
already exists an approach like this, in Business Process
Management (BPM).\cite{bn12_2} Before talking about BPM though, let's
take a glance at workflows in general.
\section{Workflows}
When talking about a framework to automate MPI experiments, we are
essentially talking about creating workflows: we connect multiple
steps, making it possible to execute them in a chain, with no or
minimal user interaction during the process. In the application level,
workflows are abstract in the sense that the workflow only describes
the goal of the experiment, its components and its dependencies. Lower
level implementation details are hidden from the user of the
workflow. This provides the possibility of changing the implementation
without having to change the high-level workflow description - as long
as the new implementation still has the properties that are written in
the description.\cite{dssbgkmvbgljk05}
In the subject of workflows, there are two main approaches that we
will discuss: scientific workflows and the previously mentioned
business workflows.
\subsection{Scientific workflows}
Scientific analyses often have to be conducted in several different
hardware and software environments. Exporting and importing data from
and to different environments can be a tedious task, slowing down the
work process, forcing researchers to divide their efforts between
computation management and their actual research. This is the main
reason scientific workflows are widely used in various different
scientific domains: they are a formalization of the ad-hoc
process that a scientist has to go through in order to get from raw
data to publishable results. Since the raw data to be analyzed can be
large, heterogeneous and complex, the process can be computationally
intensive and produce derived data formats, which is one of the main
differences between scientific and business
workflows.\cite{abjjlm04}\\
There are several tools that help in experiment design, mapping of
computing resources to the workflow and handling exceptional
situations. Some of the more well-known tools include
Kepler\cite{abjjlm04}, Pegasus\cite{dssbgkmvbgljk05} and
Taverna\cite{whfwwsdnfbbbhnvsg13}.\\
Scientific workflows are well suited for managing computation on
\emph{a priori} available data or data queried from public
databases. However, it's not well suited to cases when data
acquisition is actually part of the workflow process, or in other
words, when the source of the data is the computer system
itself. They are also data flow-driven, which is not true in the case
of our processes that we want to automate. This makes scientific
workflows not optimal for research conducted in computer science.
\subsection{Business workflows}
Business workflow management systems are usually based on agreed-upon
standards in order to facilitate communication between different
software systems and companies. The workflow logic is control
flow-driven and includes constructs to specify paths and
conditions.\cite{skd10} The top-down approach it uses is what can make
it viable in our case: in Business Process Management, the first step
is to understand the organization. Then we can model its processes as
workflows and execute and monitor them. While monitoring,
we can spot defects and work out ways to improve the organizational
activities, as well as we can redesign the processes to make them
cheaper and faster.\cite{bn12_2}\\
This approach can be utilized in the domain of experiment
orchestration, making business workflows a viable choice when
approaching our problem. An experimentation engine with the goal of
implementing this idea is XPflow\cite{bn12_2}. This engine was used to
implement the test automation framework in this thesis and will be
discussed in a bit more detail later on in this document.
\section{Reproducible research}
New scientific ideas, developments and results are only useful when
they are documented and published. It is vital that results are
announced, so others can be aware of the latest developments on their
field of research. This helps in creating a linked data cloud, used by
scientists to incorporate various output of other research into their
own, using previous results as "stepping stones" to achieve something
new.\cite{babbccrddg10} But simply publishing results is not enough in
order for others to make use of them. Besides announcing the
achievements, the other goal of scientific publications is to convince
the readers that the results it presents are correct. Besides
theoretical reasoning, papers in experimental science should provide a
documented methodology describing how the author has gotten to those
results.\cite{m10} The methodology has to be detailed and precise
enough so other researchers can repeat the same steps, thus
reproducing the same results. This is vital in order to provide the
possibility to verify those results and to fully understand
them. We have to note that in a reproduced experiment, it's not the
\emph{raw results} that need to be identical: they merely need to fit
within a statistical margin, compared to the original results, so that
the \emph{conclusions} derived from them can be the same.\\
Reproducing the results also makes for a starting point for
further development, as the described methods used for reproduction
can be extended to achieve something more or something different in
the same area of research, or repurposed to gain useful results in a
completely different area. Reproducibility is a relevant concern in
the case of SMPI and one of the main goals of the testing and
validation framework is to make developments in the area.\\[0.5cm]
In this chapter, we reviewed the background material to the general
concepts related to this thesis. We talked about the MPI inter-process
communication API and its two different implementations: OpenMPI and
MPICH. Then we talked about modeling and simulation in general and how
simulation is advantageous in certain situations. Relating to
simulation, we talked about SimGrid, a multi-purpose simulation
framework. Then we talked about the project that interconnects these
concepts: SMPI, which is a tool that is able to simulate MPI programs
on SimGrid. We talked about its features and how it's been previously
validated with extensive testing. Since testing is currently a tedious
multi-step process and continuous validation is necessary, the project
is in need of a test automation framework to help with the development
process. Also, such a framework would make documented experiments
repeatable, as well as the results reproducible, which is an important
concern as well. The subject of this framework is connected to the
subject of workflows, which we discussed in some detail in the section
after. We discussed workflows in general, as well as scientific and
business workflows. We concluded that business workflows are more
suitable for our current problem because of its top-down, control
flow-driven approach. We briefly mentioned the XPflow experimentation
engine, which is based on this approach and is utilized in
implementing the framework. In the end, we also discussed the
importance of reproducible research, which is another important aspect
in our motivation for wanting to create the automation framework.
